{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLiHbF5NEpjDPelOZOIXW7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavya2299/PrivyAI/blob/master/Network%20with%20both%20discriminator%20and%20generator\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BedTZCC9BHwI",
        "colab_type": "text"
      },
      "source": [
        "###Initiation\n",
        "*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eChFOeykA7-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 64\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# get the training datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "\n",
        "# prepare data loader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4NsSWimDebb",
        "colab_type": "text"
      },
      "source": [
        "###Visualize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TouEuOFhBkPj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "503e5f5a-dc6d-4595-e780-5dd53f43ef13"
      },
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig = plt.figure(figsize = (3,3)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f29e4bf9320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAADDCAYAAAAyYdXtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAALpUlEQVR4nO3dbYxU9RXH8d8RywspihvTlSAUIQaD\nxG4TBWNJlVgqNBhcNcRNbEgg4As2wcaQEt6obTCkom2JpJGmKCQWMVHLSkzBAEIbGyIiPmGpxNi4\nBEEDyIMPBDh9MXftevY/7Ow8z/D9JGZmzt6993+Dv9x7/3P3XHN3Afi/i2o9AKDeEAogIBRAQCiA\ngFAAAaEAgpJCYWbTzGyfme03s8XlGhRQS1bs9xRmNkjSfyRNldQt6Q1JHe6+9zy/w5ciqBvubql6\nKUeKiZL2u/tH7n5a0nOSZpawPqAulBKKEZI+6fW5O6sBDe3iSm/AzOZLml/p7QDlUkooDkga2evz\nVVntO9x9laRVEtcUaAylnD69IekaM7vazAZLuldSV3mGBdRO0UcKdz9jZp2SNkkaJGm1u79ftpEB\nNVL0lGxRG+P0CXWkElOyQFMiFEBAKICAUAABoQACQgEEhAIICAUQEAogIBRAQCiAgFAAAaEAAkIB\nBIQCCAgFEBAKICAUQEAogIBQAEFJzdDM7GNJJySdlXTG3W8ox6AuNIMGDUrWL7vssrKsv7OzM1m/\n5JJL+tTGjRuXXHbBggXJ+vLly5P1jo6OZP3rr79O1pctW5asP/LII8l6JZWjQ+AUd/+8DOsB6gKn\nT0BQaihc0mYzezPrGQs0vFJPnya7+wEz+4GkV83s3+6+o/cCNFhGoynpSOHuB7LXw5JeUu6ZFXGZ\nVe5+AxfhaBRFHynMbIiki9z9RPb+55J+U7aR1ZlRo0Yl64MHD07Wb7755j61yZMnJ5cdNmxYsn73\n3XcXOLry6e7uTtZXrFiRrLe3tyfrJ06cSNbffvvtZH379u0FjK46Sjl9apX0kpn1rOev7v73sowK\nqKFSuo5/JOlHZRwLUBeYkgUCQgEEhAIIeGhL0NbWlqxv3bo1WS/X/Um1cO7cuT61OXPmJJc9efLk\ngNZ98ODBZP3o0aPJ+r59+wa0/nLgoS1AgQgFEBAKICAUQEAogIDZp6ClpSVZ37lzZ7I+ZsyYSg4n\nKd9Yjh07lqxPmTIlWT99+nSfWiPPpg0Us09AgQgFEBAKICAUQEAogKAcLW6aypEjR5L1RYsWJesz\nZsxI1t96660+tXx/vZbPnj17kvWpU6cm66dOnUrWr7vuumR94cKFAxrPhYIjBRAQCiAgFEBAKICg\n31CY2WozO2xm7/WqtZjZq2b2YfZ6eWWHCVRPv/c+mdlPJZ2UtNbdJ2S130k64u7LzGyxpMvd/df9\nbqwB7n0aqEsvvTRZT/U9euqpp5LLzp07N1m/7777kvV169YVODqcT9H3PmVtMOM85UxJa7L3ayTd\nWdLogDpS7DVFq7v3/BHup8o1RgOaQslf3rm7n++0iAbLaDTFHikOmdlwScpeD+dbkAbLaDTFHim6\nJM2WtCx73VC2ETWY48ePF7zsF198MaB1z5s3L1lfv359sp5qWYOBK2RKdp2kf0kaZ2bdZjZXuTBM\nNbMPJf0s+ww0hX6PFO6efqKfdFuZxwLUBb7RBgJCAQSEAghocVNFQ4YMSdZffvnlZP2WW25J1qdP\nn56sb968ubiBXaBocQMUiFAAAaEAAkIBBIQCCJh9qgNjx45N1nfv3p2s52ukvG3btmR9165dyfrK\nlSv71Kr5/0OtMfsEFIhQAAGhAAJCAQSEAgiYfapj7e3tyfrTTz+drA8dOnRA61+yZEmf2tq1a5PL\n5ntYfCNj9gkoEKEAAkIBBIQCCAgFEBTSYHm1pBmSDvdqsPywpHmSPssWW+Lur/S7MWafymLChAnJ\n+hNPPJGs33Zb4Y1X8jWBXrp0abJ+4MCBgtddb0qZfXpG0rRE/ffu3pb9128ggEZRbNdxoGmVck3R\naWbvZA91yfvQFjObb2a7zCx9/zJQZ4oNxZ8kjZXUJumgpMfzLUiDZTSaokLh7ofc/ay7n5P0Z0kT\nyzssoHYKuvfJzEZL2thr9ml4z0NbzOxXkia5+70FrIfZpwoaNmxYsn7HHXck66l7qMySEzLaunVr\nsp7vQfeNIN/sU78NlrOu47dKusLMuiU9JOlWM2uT5JI+lnR/2UYK1FixXcf/UoGxAHWBb7SBgFAA\nAaEAAv7y7gL2zTff9KldfHH6MvPMmTPJ+u23356sv/baa0WPq1r4yzugQIQCCAgFEBAKICj24fKo\noeuvvz5Zv+eee5L1G2+8MVnPd1Gdsnfv3mR9x44dBa+jUXCkAAJCAQSEAggIBRAQCiBg9qkOjBs3\nLlnv7OxM1u+6665k/corryx5LGfPnk3W8zVYPnfuXMnbrDccKYCAUAABoQACQgEEhAIICunmMVLS\nWkmtynXvWOXufzSzFknrJY1WrqPHLHc/WrmhNpbUTFBHR6oHRP5ZptGjR5dzSH2kHjqfr5FyV1dX\nRcdSTwo5UpyR9KC7j5d0k6QFZjZe0mJJW9z9Gklbss9AwyukwfJBd9+dvT8h6QNJIyTNlLQmW2yN\npDsrNUigmgb05V3WKfDHknZKau3pEijpU+VOr1K/M1/S/OKHCFRXwRfaZvZ9SS9IesDdj/f+mee6\nHySbEtBgGY2moFCY2feUC8Sz7v5iVj5kZsOznw+XdLgyQwSqq5DZJ1OuTeYH7t77+VFdkmZLWpa9\nbqjICOtEa2vy7FDjx49P1p988sk+tWuvvbasY4p27tyZrD/22GPJ+oYNff/JmvFepoEq5JriJ5J+\nKeldM9uT1ZYoF4bnzWyupP9KmlWZIQLVVUiD5X9KSvdnlwp/wiDQIPhGGwgIBRAQCiC4YP/yrqWl\nJVnP93D1tra2ZH3MmDFlG1P0+uuvJ+uPP55+7uamTZuS9a+++qpsY7oQcKQAAkIBBIQCCAgFEBAK\nIGia2adJkyYl64sWLUrWJ06cmKyPGDGibGOKvvzyy2R9xYoVyfqjjz6arJ86dapsY0JfHCmAgFAA\nAaEAAkIBBIQCCJpm9qm9vX1A9YHK98y3jRs3Juuph7Hnu2fp2LFjxQ8MZceRAggIBRAQCiAgFEBg\nuT5m51kgf4PlhyXNk/RZtugSd3+ln3Wdf2NAFbl7siFHIaEYLmm4u+82s6GS3lSub+wsSSfdfXmh\ngyAUqCf5QlFIi5uDkg5m70+YWU+DZaApDeiaIjRYlqROM3vHzFab2eV5fme+me0ys74PQwDqUL+n\nT98umGuwvF3SUnd/0cxaJX2u3HXGb5U7xZrTzzo4fULdKPqaQvq2wfJGSZtCP9men4+WtNHdJ/Sz\nHkKBupEvFP2ePuVrsNzTcTzTLum9UgcJ1INCZp8mS/qHpHcl9bSkXiKpQ1KbcqdPH0u6v9dDXPKt\niyMF6kZJp0/lQihQT4o+fQIuNIQCCAgFEBAKICAUQEAogIBQAAGhAAJCAQTVbnHzuXLP3JakK7LP\nzY79rE8/zPeDqt7m8Z0Nm+1y9xtqsvEqYj8bD6dPQEAogKCWoVhVw21XE/vZYGp2TQHUK06fgKDq\noTCzaWa2z8z2m9niam+/krKuJofN7L1etRYze9XMPsxek11PGomZjTSzbWa218zeN7OFWb0p9rWq\noTCzQZJWSpouabykDjMbX80xVNgzkqaF2mJJW9z9Gklbss+N7oykB919vKSbJC3I/h2bYl+rfaSY\nKGm/u3/k7qclPSdpZpXHUDHuvkPSkVCeKWlN9n6Nct0VG5q7H3T33dn7E5J6GuQ1xb5WOxQjJH3S\n63O3mr/bYGuvhg6fKteTt2mEBnlNsa9caFeR56b6mma6L2uQ94KkB9z9eO+fNfK+VjsUBySN7PX5\nqqzWzA719MjKXg/XeDxlkTXIe0HSs+7+YlZuin2tdijekHSNmV1tZoMl3Supq8pjqLYuSbOz97Ml\nbajhWMoiX4M8Ncm+Vv3LOzP7haQ/SBokabW7L63qACrIzNZJulW5O0YPSXpI0t8kPS9plHJ3CM9y\n93gx3lDO0yBvp5pgX/lGGwi40AYCQgEEhAIICAUQEAogIBRAQCiAgFAAwf8Ac0KUEmzQH7gAAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avhnh-YeBxL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_dim, output_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        # define hidden linear layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
        "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
        "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
        "        \n",
        "        # final fully-connected layer\n",
        "        self.fc4 = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "        # dropout layer \n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # flatten image\n",
        "        x = x.view(-1, 28*28)\n",
        "        # all hidden layers\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        # final layer\n",
        "        out = self.fc4(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZl36u7WB2cE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_dim, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        # define hidden linear layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
        "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
        "        \n",
        "        # final fully-connected layer\n",
        "        self.fc4 = nn.Linear(hidden_dim*4, output_size)\n",
        "        \n",
        "        # dropout layer \n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # all hidden layers\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = self.dropout(x)\n",
        "        # final layer with tanh applied\n",
        "        out = F.tanh(self.fc4(x))\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwUb5gJAB86Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Discriminator hyperparams\n",
        "\n",
        "# Size of input image to discriminator (28*28)\n",
        "input_size = 784\n",
        "# Size of discriminator output (real or fake)\n",
        "d_output_size = 1\n",
        "# Size of last hidden layer in the discriminator\n",
        "d_hidden_size = 32\n",
        "\n",
        "# Generator hyperparams\n",
        "\n",
        "# Size of latent vector to give to generator\n",
        "z_size = 100\n",
        "# Size of discriminator output (generated image)\n",
        "g_output_size = 784\n",
        "# Size of first hidden layer in the generator\n",
        "g_hidden_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVnNZBPtCEvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "ff2ed0aa-b86e-4d4c-fb7e-905cf1762273"
      },
      "source": [
        "# instantiate discriminator and generator\n",
        "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
        "G = Generator(z_size, g_hidden_size, g_output_size)\n",
        "\n",
        "# check that they are as you expect\n",
        "print(D)\n",
        "print()\n",
        "print(G)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n",
            "\n",
            "Generator(\n",
            "  (fc1): Linear(in_features=100, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=784, bias=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeyQFPu0CI8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate losses\n",
        "def real_loss(D_out, smooth=False):\n",
        "    batch_size = D_out.size(0)\n",
        "    # label smoothing\n",
        "    if smooth:\n",
        "        # smooth, real labels = 0.9\n",
        "        labels = torch.ones(batch_size)*0.9\n",
        "    else:\n",
        "        labels = torch.ones(batch_size) # real labels = 1\n",
        "        \n",
        "    # numerically stable loss\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculate loss\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.zeros(batch_size) # fake labels = 0\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # calculate loss\n",
        "    loss = criterion(D_out.squeeze(), labels)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiri-1cdCMtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84c5086a-271f-47e0-a2a9-419446781884"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Optimizers\n",
        "lr = 0.002\n",
        "\n",
        "# Create optimizers for the discriminator and generator\n",
        "d_optimizer = optim.Adam(D.parameters(), lr)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)\n",
        "import pickle as pkl\n",
        "\n",
        "# training hyperparams\n",
        "num_epochs = 100\n",
        "\n",
        "# keep track of loss and generated, \"fake\" samples\n",
        "samples = []\n",
        "losses = []\n",
        "\n",
        "print_every = 400\n",
        "\n",
        "# Get some fixed data for sampling. These are images that are held\n",
        "# constant throughout training, and allow us to inspect the model's performance\n",
        "sample_size=16\n",
        "fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "fixed_z = torch.from_numpy(fixed_z).float()\n",
        "\n",
        "# train the network\n",
        "D.train()\n",
        "G.train()\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    for batch_i, (real_images, _) in enumerate(train_loader):\n",
        "                \n",
        "        batch_size = real_images.size(0)\n",
        "        \n",
        "        ## Important rescaling step ## \n",
        "        real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n",
        "                # ============================================\n",
        "        #            TRAIN THE DISCRIMINATOR\n",
        "        # ============================================\n",
        "        \n",
        "        d_optimizer.zero_grad()\n",
        "        \n",
        "        # 1. Train with real images\n",
        "\n",
        "        # Compute the discriminator losses on real images \n",
        "        # smooth the real labels\n",
        "        D_real = D(real_images)\n",
        "        d_real_loss = real_loss(D_real, smooth=True)\n",
        "        \n",
        "        # 2. Train with fake images\n",
        "        \n",
        "        # Generate fake images\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "        z = torch.from_numpy(z).float()\n",
        "        fake_images = G(z)\n",
        "        \n",
        "        # Compute the discriminator losses on fake images        \n",
        "        D_fake = D(fake_images)\n",
        "        d_fake_loss = fake_loss(D_fake)\n",
        "        \n",
        "        # add up loss and perform backprop\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        \n",
        "        # =========================================\n",
        "        #            TRAIN THE GENERATOR\n",
        "        # =========================================\n",
        "        g_optimizer.zero_grad()\n",
        "        \n",
        "        # 1. Train with fake images and flipped labels\n",
        "                # Generate fake images\n",
        "        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "        z = torch.from_numpy(z).float()\n",
        "        fake_images = G(z)\n",
        "        \n",
        "        # Compute the discriminator losses on fake images \n",
        "        # using flipped labels!\n",
        "        D_fake = D(fake_images)\n",
        "        g_loss = real_loss(D_fake) # use real loss to flip labels\n",
        "        \n",
        "        # perform backprop\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # Print some loss stats\n",
        "        if batch_i % print_every == 0:\n",
        "            # print discriminator and generator loss\n",
        "            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
        "                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "    \n",
        "    ## AFTER EACH EPOCH##\n",
        "    # append discriminator loss and generator loss\n",
        "    losses.append((d_loss.item(), g_loss.item()))\n",
        "    \n",
        "    # generate and save sample, fake images\n",
        "    G.eval() # eval mode for generating samples\n",
        "    samples_z = G(fixed_z)\n",
        "    samples.append(samples_z)\n",
        "    G.train() # back to train mode\n",
        "\n",
        "\n",
        "# Save training generator samples\n",
        "with open('train_samples.pkl', 'wb') as f:\n",
        "    pkl.dump(samples, f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [    1/  100] | d_loss: 1.4236 | g_loss: 0.6484\n",
            "Epoch [    1/  100] | d_loss: 1.0619 | g_loss: 2.4894\n",
            "Epoch [    1/  100] | d_loss: 1.2963 | g_loss: 0.9141\n",
            "Epoch [    2/  100] | d_loss: 1.3753 | g_loss: 0.7514\n",
            "Epoch [    2/  100] | d_loss: 1.2448 | g_loss: 1.2892\n",
            "Epoch [    2/  100] | d_loss: 1.5030 | g_loss: 1.3787\n",
            "Epoch [    3/  100] | d_loss: 1.3057 | g_loss: 1.0239\n",
            "Epoch [    3/  100] | d_loss: 1.1664 | g_loss: 1.0841\n",
            "Epoch [    3/  100] | d_loss: 1.3070 | g_loss: 0.8787\n",
            "Epoch [    4/  100] | d_loss: 1.2490 | g_loss: 1.0500\n",
            "Epoch [    4/  100] | d_loss: 1.6565 | g_loss: 0.8713\n",
            "Epoch [    4/  100] | d_loss: 1.3951 | g_loss: 0.9418\n",
            "Epoch [    5/  100] | d_loss: 1.2644 | g_loss: 1.1631\n",
            "Epoch [    5/  100] | d_loss: 0.9300 | g_loss: 1.9049\n",
            "Epoch [    5/  100] | d_loss: 0.9675 | g_loss: 1.7496\n",
            "Epoch [    6/  100] | d_loss: 1.2775 | g_loss: 0.9810\n",
            "Epoch [    6/  100] | d_loss: 1.1529 | g_loss: 1.2224\n",
            "Epoch [    6/  100] | d_loss: 1.2608 | g_loss: 1.1318\n",
            "Epoch [    7/  100] | d_loss: 1.1724 | g_loss: 1.1550\n",
            "Epoch [    7/  100] | d_loss: 1.1218 | g_loss: 1.6216\n",
            "Epoch [    7/  100] | d_loss: 1.2486 | g_loss: 1.0158\n",
            "Epoch [    8/  100] | d_loss: 1.0147 | g_loss: 1.6975\n",
            "Epoch [    8/  100] | d_loss: 1.2604 | g_loss: 1.7170\n",
            "Epoch [    8/  100] | d_loss: 1.1499 | g_loss: 1.0328\n",
            "Epoch [    9/  100] | d_loss: 1.2436 | g_loss: 1.1421\n",
            "Epoch [    9/  100] | d_loss: 1.2154 | g_loss: 1.1178\n",
            "Epoch [    9/  100] | d_loss: 1.1615 | g_loss: 1.0418\n",
            "Epoch [   10/  100] | d_loss: 1.2928 | g_loss: 0.9006\n",
            "Epoch [   10/  100] | d_loss: 1.3614 | g_loss: 1.3002\n",
            "Epoch [   10/  100] | d_loss: 1.0986 | g_loss: 1.6417\n",
            "Epoch [   11/  100] | d_loss: 1.2631 | g_loss: 1.1470\n",
            "Epoch [   11/  100] | d_loss: 1.1823 | g_loss: 1.1164\n",
            "Epoch [   11/  100] | d_loss: 1.2298 | g_loss: 1.7346\n",
            "Epoch [   12/  100] | d_loss: 1.2717 | g_loss: 1.0360\n",
            "Epoch [   12/  100] | d_loss: 1.2148 | g_loss: 1.5895\n",
            "Epoch [   12/  100] | d_loss: 1.3285 | g_loss: 1.0715\n",
            "Epoch [   13/  100] | d_loss: 1.2382 | g_loss: 1.0903\n",
            "Epoch [   13/  100] | d_loss: 1.1967 | g_loss: 1.4447\n",
            "Epoch [   13/  100] | d_loss: 1.1687 | g_loss: 1.3044\n",
            "Epoch [   14/  100] | d_loss: 1.3488 | g_loss: 1.0862\n",
            "Epoch [   14/  100] | d_loss: 1.4225 | g_loss: 1.1259\n",
            "Epoch [   14/  100] | d_loss: 1.1844 | g_loss: 1.1206\n",
            "Epoch [   15/  100] | d_loss: 1.2379 | g_loss: 1.2440\n",
            "Epoch [   15/  100] | d_loss: 1.2960 | g_loss: 1.1355\n",
            "Epoch [   15/  100] | d_loss: 1.4303 | g_loss: 1.0235\n",
            "Epoch [   16/  100] | d_loss: 1.1759 | g_loss: 0.9667\n",
            "Epoch [   16/  100] | d_loss: 1.3259 | g_loss: 1.2448\n",
            "Epoch [   16/  100] | d_loss: 1.0756 | g_loss: 1.2414\n",
            "Epoch [   17/  100] | d_loss: 1.3328 | g_loss: 1.0136\n",
            "Epoch [   17/  100] | d_loss: 1.3455 | g_loss: 1.3066\n",
            "Epoch [   17/  100] | d_loss: 1.2324 | g_loss: 1.1669\n",
            "Epoch [   18/  100] | d_loss: 1.2563 | g_loss: 1.2777\n",
            "Epoch [   18/  100] | d_loss: 1.2401 | g_loss: 0.8728\n",
            "Epoch [   18/  100] | d_loss: 1.3130 | g_loss: 0.9737\n",
            "Epoch [   19/  100] | d_loss: 1.2725 | g_loss: 0.9934\n",
            "Epoch [   19/  100] | d_loss: 1.2903 | g_loss: 0.9952\n",
            "Epoch [   19/  100] | d_loss: 1.2473 | g_loss: 1.2319\n",
            "Epoch [   20/  100] | d_loss: 1.2458 | g_loss: 1.2034\n",
            "Epoch [   20/  100] | d_loss: 1.2881 | g_loss: 1.0029\n",
            "Epoch [   20/  100] | d_loss: 1.2454 | g_loss: 1.0809\n",
            "Epoch [   21/  100] | d_loss: 1.4047 | g_loss: 1.1895\n",
            "Epoch [   21/  100] | d_loss: 1.2917 | g_loss: 0.9291\n",
            "Epoch [   21/  100] | d_loss: 1.2654 | g_loss: 1.1226\n",
            "Epoch [   22/  100] | d_loss: 1.2838 | g_loss: 0.8343\n",
            "Epoch [   22/  100] | d_loss: 1.2431 | g_loss: 1.0552\n",
            "Epoch [   22/  100] | d_loss: 1.3330 | g_loss: 0.9245\n",
            "Epoch [   23/  100] | d_loss: 1.2863 | g_loss: 0.9674\n",
            "Epoch [   23/  100] | d_loss: 1.2056 | g_loss: 1.1644\n",
            "Epoch [   23/  100] | d_loss: 1.2809 | g_loss: 1.0904\n",
            "Epoch [   24/  100] | d_loss: 1.3617 | g_loss: 0.8896\n",
            "Epoch [   24/  100] | d_loss: 1.2834 | g_loss: 1.1650\n",
            "Epoch [   24/  100] | d_loss: 1.3539 | g_loss: 0.8783\n",
            "Epoch [   25/  100] | d_loss: 1.2881 | g_loss: 1.4102\n",
            "Epoch [   25/  100] | d_loss: 1.1473 | g_loss: 1.4678\n",
            "Epoch [   25/  100] | d_loss: 1.4209 | g_loss: 1.1070\n",
            "Epoch [   26/  100] | d_loss: 1.3282 | g_loss: 1.1124\n",
            "Epoch [   26/  100] | d_loss: 1.2285 | g_loss: 0.9464\n",
            "Epoch [   26/  100] | d_loss: 1.3855 | g_loss: 0.9162\n",
            "Epoch [   27/  100] | d_loss: 1.2519 | g_loss: 0.8374\n",
            "Epoch [   27/  100] | d_loss: 1.3050 | g_loss: 0.8445\n",
            "Epoch [   27/  100] | d_loss: 1.2706 | g_loss: 0.9812\n",
            "Epoch [   28/  100] | d_loss: 1.3084 | g_loss: 1.0903\n",
            "Epoch [   28/  100] | d_loss: 1.2178 | g_loss: 0.9957\n",
            "Epoch [   28/  100] | d_loss: 1.3707 | g_loss: 1.2814\n",
            "Epoch [   29/  100] | d_loss: 1.3176 | g_loss: 0.9776\n",
            "Epoch [   29/  100] | d_loss: 1.1585 | g_loss: 1.1308\n",
            "Epoch [   29/  100] | d_loss: 1.3133 | g_loss: 1.0280\n",
            "Epoch [   30/  100] | d_loss: 1.2726 | g_loss: 0.9061\n",
            "Epoch [   30/  100] | d_loss: 1.2093 | g_loss: 0.9154\n",
            "Epoch [   30/  100] | d_loss: 1.2652 | g_loss: 0.9904\n",
            "Epoch [   31/  100] | d_loss: 1.3225 | g_loss: 1.0484\n",
            "Epoch [   31/  100] | d_loss: 1.2806 | g_loss: 0.8266\n",
            "Epoch [   31/  100] | d_loss: 1.3838 | g_loss: 0.9668\n",
            "Epoch [   32/  100] | d_loss: 1.3397 | g_loss: 1.0918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8oYKmMzCsls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "losses = np.array(losses)\n",
        "plt.plot(losses.T[0], label='Discriminator')\n",
        "plt.plot(losses.T[1], label='Generator')\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "# helper function for viewing a list of passed in sample images\n",
        "def view_samples(epoch, samples):\n",
        "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
        "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
        "        img = img.detach()\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
        "\n",
        "with open('train_samples.pkl', 'rb') as f:\n",
        "    samples = pkl.load(f)\n",
        "\n",
        "# -1 indicates final epoch's samples (the last in the list)\n",
        "view_samples(-1, samples)\n",
        "\n",
        "rows = 10 # split epochs into 10, so 100/10 = every 10 epochs\n",
        "cols = 6\n",
        "fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n",
        "\n",
        "for sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\n",
        "    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n",
        "        img = img.detach()\n",
        "        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.yaxis.set_visible(False)\n",
        "\n",
        "# randomly generated, new latent vectors\n",
        "sample_size=16\n",
        "rand_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "rand_z = torch.from_numpy(rand_z).float()\n",
        "\n",
        "G.eval() # eval mode\n",
        "# generated samples\n",
        "rand_images = G(rand_z)\n",
        "\n",
        "# 0 indicates the first set of samples in the passed in list\n",
        "# and we only have one batch of samples, here\n",
        "view_samples(0, [rand_images])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}